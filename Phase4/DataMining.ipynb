{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "current_path = os.getcwd()\n",
    "data_sources = os.path.join(current_path, '..', 'Data Sources')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "#datasets we're using\n",
    "real_estate = pd.read_csv(os.path.join(data_sources, 'RealEstate_Sample_Wards.csv'))\n",
    "real_estate.rename(columns={\"Ward_Index\": \"Ward_ID\"},inplace=True)\n",
    "real_estate['Ward_ID'] = real_estate['Ward_ID'].fillna(-1) #to fill any missing values\n",
    "real_estate['Ward_ID'] = real_estate['Ward_ID'].astype(int)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "transportation = pd.read_csv(os.path.join(data_sources, 'Transportation_Sample_Wards.csv'))\n",
    "transportation.rename(columns={\"Ward_Index\": \"Ward_ID\"},inplace=True)\n",
    "transportation['Ward_ID'] = transportation['Ward_ID'].fillna(-1)\n",
    "transportation['Ward_ID'] = transportation['Ward_ID'].astype(int)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "amenities = pd.read_csv(os.path.join(data_sources, 'Amenities_Sample_Ward.csv'))\n",
    "amenities.rename(columns={\"Ward_Index\": \"Ward_ID\"},inplace=True)\n",
    "amenities['Ward_ID'] = amenities['Ward_ID'].fillna(-1)\n",
    "amenities['Ward_ID'] = amenities['Ward_ID'].astype(int)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "social_dev = pd.read_csv(os.path.join(data_sources, 'Social_Development_Sample_Wards.csv'))\n",
    "social_dev.rename(columns={\"Ward_Index\": \"Ward_ID\"},inplace=True)\n",
    "social_dev['Ward_ID'] = social_dev['Ward_ID'].fillna(-1)\n",
    "social_dev['Ward_ID'] = social_dev['Ward_ID'].astype(int)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "#data cleaning from Phase 2\n",
    "education_data2016 = pd.read_csv(\n",
    "    os.path.join(data_sources,\"WardProfile2016.csv\"),\n",
    "    skiprows=range(833),\n",
    "    nrows=16,\n",
    "    header=0,\n",
    "    encoding=\"ISO-8859-1\",\n",
    "    low_memory=False,\n",
    ")\n",
    "education_data2021 = pd.read_csv(\n",
    "    os.path.join(data_sources,\"WardProfile2021.csv\"),\n",
    "    skiprows=range(978),\n",
    "    nrows=17,\n",
    "    header=0,\n",
    "    encoding=\"ISO-8859-1\",\n",
    "    low_memory=False,\n",
    ")\n",
    "\n",
    "# rename 'Education' column to 'Education_Level' before melting\n",
    "education_data2016.rename(columns={\"Education\": \"Education_Level\"}, inplace=True)\n",
    "education_data2021.rename(columns={\"Education\": \"Education_Level\"}, inplace=True)\n",
    "\n",
    "# filter out rows where 'Education_Level' column is not empty\n",
    "education_data2016 = education_data2016[education_data2016[\"Education_Level\"].notna()]\n",
    "education_data2021 = education_data2021[education_data2021[\"Education_Level\"].notna()]\n",
    "\n",
    "# remove all spaces in education_level column\n",
    "education_data2016[\"Education_Level\"] = education_data2016[\n",
    "    \"Education_Level\"\n",
    "].str.strip()\n",
    "education_data2021[\"Education_Level\"] = education_data2021[\n",
    "    \"Education_Level\"\n",
    "].str.strip()\n",
    "\n",
    "# Melt the DataFrame to get 'Ward_ID', 'Education_Level', and 'Population' columns\n",
    "education_data2016 = pd.melt(\n",
    "    education_data2016,\n",
    "    id_vars=[\"Education_Level\"],\n",
    "    var_name=\"Ward_ID\",\n",
    "    value_name=\"Population\",\n",
    ")\n",
    "education_data2016[\"Year\"] = 2016\n",
    "education_data2021 = pd.melt(\n",
    "    education_data2021,\n",
    "    id_vars=[\"Education_Level\"],\n",
    "    var_name=\"Ward_ID\",\n",
    "    value_name=\"Population\",\n",
    ")\n",
    "education_data2021[\"Year\"] = 2021\n",
    "\n",
    "# merge the datasets\n",
    "Education = pd.concat(\n",
    "    [education_data2016, education_data2021], ignore_index=True\n",
    ")\n",
    "\n",
    "# change data types\n",
    "Education[\"Population\"] = Education[\"Population\"].astype(int)\n",
    "Education[\"Education_Level\"] = Education[\"Education_Level\"].astype(\n",
    "    str\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "# INDUSTRY DIMENSION\n",
    "industry_data2016 = pd.read_csv(\n",
    "    os.path.join(data_sources,\"WardProfile2016.csv\"),\n",
    "    skiprows=range(1176),\n",
    "    nrows=22,\n",
    "    header=0,\n",
    "    encoding=\"ISO-8859-1\",\n",
    "    low_memory=False,\n",
    ")\n",
    "industry_data2021 = pd.read_csv(\n",
    "    os.path.join(data_sources,\"WardProfile2021.csv\"),\n",
    "    skiprows=range(1310),\n",
    "    nrows=22,\n",
    "    header=0,\n",
    "    encoding=\"ISO-8859-1\",\n",
    "    low_memory=False,\n",
    ")\n",
    "\n",
    "# Filter out rows where 'Industry' column is not empty\n",
    "industry_data2016 = industry_data2016[industry_data2016[\"Industry\"].notna()]\n",
    "industry_data2021 = industry_data2021[industry_data2021[\"Industry\"].notna()]\n",
    "\n",
    "# Removing the numbers and spaces before each industry type\n",
    "industry_data2016[\"Industry\"] = industry_data2016[\"Industry\"].str.strip()\n",
    "industry_data2021[\"Industry\"] = industry_data2021[\"Industry\"].str.strip()\n",
    "industry_data2016[\"Industry\"] = industry_data2016[\"Industry\"].str.replace(\n",
    "    r\"^\\s*\\d+(-\\d+)?\\s+\", \"\", regex=True\n",
    ")\n",
    "industry_data2021[\"Industry\"] = industry_data2021[\"Industry\"].str.replace(\n",
    "    r\"^\\s*\\d+(-\\d+)?\\s+\", \"\", regex=True\n",
    ")\n",
    "\n",
    "# Melt the DataFrame to get 'Ward_ID', 'Industry', and 'Population' columns\n",
    "industry_data2016 = pd.melt(\n",
    "    industry_data2016, id_vars=[\"Industry\"], var_name=\"Ward_ID\", value_name=\"Population\"\n",
    ")\n",
    "industry_data2016[\"Year\"] = 2016\n",
    "industry_data2021 = pd.melt(\n",
    "    industry_data2021, id_vars=[\"Industry\"], var_name=\"Ward_ID\", value_name=\"Population\"\n",
    ")\n",
    "industry_data2021[\"Year\"] = 2021\n",
    "\n",
    "# merge the datasets\n",
    "IndustryDimension = pd.concat([industry_data2016, industry_data2021], ignore_index=True)\n",
    "\n",
    "# change data types\n",
    "IndustryDimension[\"Population\"] = IndustryDimension[\"Population\"].astype(int)\n",
    "IndustryDimension[\"Ward_ID\"] = IndustryDimension[\"Ward_ID\"].astype(str)\n",
    "IndustryDimension[\"Industry\"] = IndustryDimension[\"Industry\"].astype(str)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/serenaiyoha/Documents/CSI4142/CSI4142-Gentrification-Project/Phase4/../Data Sources/2021.csv'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m/var/folders/73/svsm8vw94tzb_w86qcpwfh2h0000gn/T/ipykernel_98687/394517476.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      8\u001B[0m     \u001B[0mlow_memory\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mFalse\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      9\u001B[0m )\n\u001B[0;32m---> 10\u001B[0;31m income_data2021 = pd.read_csv(\n\u001B[0m\u001B[1;32m     11\u001B[0m     \u001B[0mos\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjoin\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdata_sources\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\"2021.csv\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     12\u001B[0m     \u001B[0mskiprows\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mrange\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m1389\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/util/_decorators.py\u001B[0m in \u001B[0;36mwrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    309\u001B[0m                     \u001B[0mstacklevel\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mstacklevel\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    310\u001B[0m                 )\n\u001B[0;32m--> 311\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mfunc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    312\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    313\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mwrapper\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001B[0m in \u001B[0;36mread_csv\u001B[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001B[0m\n\u001B[1;32m    676\u001B[0m     \u001B[0mkwds\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mupdate\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mkwds_defaults\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    677\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 678\u001B[0;31m     \u001B[0;32mreturn\u001B[0m \u001B[0m_read\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfilepath_or_buffer\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mkwds\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    679\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    680\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001B[0m in \u001B[0;36m_read\u001B[0;34m(filepath_or_buffer, kwds)\u001B[0m\n\u001B[1;32m    573\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    574\u001B[0m     \u001B[0;31m# Create the parser.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 575\u001B[0;31m     \u001B[0mparser\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mTextFileReader\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfilepath_or_buffer\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwds\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    576\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    577\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0mchunksize\u001B[0m \u001B[0;32mor\u001B[0m \u001B[0miterator\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, f, engine, **kwds)\u001B[0m\n\u001B[1;32m    930\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    931\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mhandles\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mIOHandles\u001B[0m \u001B[0;34m|\u001B[0m \u001B[0;32mNone\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 932\u001B[0;31m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_engine\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_make_engine\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mf\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mengine\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    933\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    934\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mclose\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001B[0m in \u001B[0;36m_make_engine\u001B[0;34m(self, f, engine)\u001B[0m\n\u001B[1;32m   1214\u001B[0m             \u001B[0;31m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1215\u001B[0m             \u001B[0;31m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1216\u001B[0;31m             self.handles = get_handle(  # type: ignore[call-overload]\n\u001B[0m\u001B[1;32m   1217\u001B[0m                 \u001B[0mf\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1218\u001B[0m                 \u001B[0mmode\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/common.py\u001B[0m in \u001B[0;36mget_handle\u001B[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001B[0m\n\u001B[1;32m    784\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mioargs\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mencoding\u001B[0m \u001B[0;32mand\u001B[0m \u001B[0;34m\"b\"\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mioargs\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmode\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    785\u001B[0m             \u001B[0;31m# Encoding\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 786\u001B[0;31m             handle = open(\n\u001B[0m\u001B[1;32m    787\u001B[0m                 \u001B[0mhandle\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    788\u001B[0m                 \u001B[0mioargs\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmode\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: '/Users/serenaiyoha/Documents/CSI4142/CSI4142-Gentrification-Project/Phase4/../Data Sources/2021.csv'"
     ]
    }
   ],
   "source": [
    "# INCOME DIMENSION\n",
    "income_data2016 = pd.read_csv(\n",
    "    os.path.join(data_sources,\"WardProfile2016.csv\"),\n",
    "    skiprows=range(1252),\n",
    "    nrows=17,\n",
    "    header=0,\n",
    "    encoding=\"ISO-8859-1\",\n",
    "    low_memory=False,\n",
    ")\n",
    "income_data2021 = pd.read_csv(\n",
    "    os.path.join(data_sources,\"2021.csv\"),\n",
    "    skiprows=range(1389),\n",
    "    nrows=17,\n",
    "    header=0,\n",
    "    encoding=\"ISO-8859-1\",\n",
    "    low_memory=False,\n",
    ")\n",
    "\n",
    "# Filter out rows where 'Income' column is not NA\n",
    "income_data2016 = income_data2016[income_data2016[\"Income\"].notna()]\n",
    "income_data2021 = income_data2021[income_data2021[\"Income\"].notna()]\n",
    "\n",
    "# change value of one of income ranges\n",
    "income_data2016.loc[\n",
    "    income_data2016[\"Income\"].str.contains(\"Total - Total income groups\"), \"Income\"\n",
    "] = \"Total Income Groups\"\n",
    "income_data2021.loc[\n",
    "    income_data2021[\"Income\"].str.contains(\"Total - Total Income groups\"), \"Income\"\n",
    "] = \"Total Income Groups\"\n",
    "\n",
    "# Removing the spaces before each income\n",
    "income_data2016[\"Income\"] = income_data2016[\"Income\"].str.strip()\n",
    "income_data2021[\"Income\"] = income_data2021[\"Income\"].str.strip()\n",
    "\n",
    "# Melt the DataFrame to get 'Ward_ID', 'Income', and 'Population' columns\n",
    "income_data2016[\"Year\"] = 2016\n",
    "income_data2021[\"Year\"] = 2021\n",
    "income_data2016 = pd.melt(\n",
    "    income_data2016,\n",
    "    id_vars=[\"Income\", \"Year\"],\n",
    "    var_name=\"Ward_ID\",\n",
    "    value_name=\"Population\",\n",
    ")\n",
    "income_data2021 = pd.melt(\n",
    "    income_data2021,\n",
    "    id_vars=[\"Income\", \"Year\"],\n",
    "    var_name=\"Ward_ID\",\n",
    "    value_name=\"Population\",\n",
    ")\n",
    "columns_order = [col for col in income_data2016.columns if col != \"Year\"] + [\"Year\"]\n",
    "income_data2016 = income_data2016[columns_order]\n",
    "income_data2021 = income_data2021[columns_order]\n",
    "\n",
    "# merge the datasets\n",
    "IncomeDimension = pd.concat([income_data2016, income_data2021], ignore_index=True)\n",
    "\n",
    "# change data types\n",
    "IncomeDimension[\"Population\"] = IncomeDimension[\"Population\"].astype(int)\n",
    "IncomeDimension[\"Ward_ID\"] = IncomeDimension[\"Ward_ID\"].astype(str)\n",
    "IncomeDimension[\"Income\"] = IncomeDimension[\"Income\"].astype(str)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "# ETHNOCULTURAL DIMENSION\n",
    "ethnicity_data2016 = pd.read_csv(\n",
    "    os.path.join(data_sources,\"WardProfile2016.csv\"),\n",
    "    skiprows=range(851),\n",
    "    nrows=280,\n",
    "    header=0,\n",
    "    encoding=\"ISO-8859-1\",\n",
    "    low_memory=False,\n",
    ")\n",
    "ethnicity_data2021 = pd.read_csv(\n",
    "    os.path.join(data_sources,\"WardProfile2021.csv\"),\n",
    "    skiprows=range(1013),\n",
    "    nrows=252,\n",
    "    header=0,\n",
    "    encoding=\"ISO-8859-1\",\n",
    "    low_memory=False,\n",
    ")\n",
    "\n",
    "# Filter out rows where 'Ethnocultural' column is not NA\n",
    "ethnicity_data2016 = ethnicity_data2016[ethnicity_data2016[\"Ethnoculture\"].notna()]\n",
    "ethnicity_data2021 = ethnicity_data2021[ethnicity_data2021[\"Ethnoculture\"].notna()]\n",
    "\n",
    "# change value of one of ethnic ranges\n",
    "ethnicity_data2016.loc[\n",
    "    ethnicity_data2016[\"Ethnoculture\"].str.contains(\"Total - Ethnic origin\"),\n",
    "    \"Ethnoculture\",\n",
    "] = \"Total Ethnic Origin\"\n",
    "ethnicity_data2021.loc[\n",
    "    ethnicity_data2021[\"Ethnoculture\"].str.contains(\"Total - Ethnic origin\"),\n",
    "    \"Ethnoculture\",\n",
    "] = \"Total Ethnic Origin\"\n",
    "\n",
    "# Removing the spaces before each ethnicity\n",
    "ethnicity_data2016[\"Ethnoculture\"] = ethnicity_data2016[\"Ethnoculture\"].str.strip()\n",
    "ethnicity_data2021[\"Ethnoculture\"] = ethnicity_data2021[\"Ethnoculture\"].str.strip()\n",
    "\n",
    "# Melt the DataFrame to get 'Ward_ID', 'Ethnocultural', and 'Population' columns\n",
    "ethnicity_data2016[\"Year\"] = 2016\n",
    "ethnicity_data2021[\"Year\"] = 2021\n",
    "ethnicity_data2016 = pd.melt(\n",
    "    ethnicity_data2016,\n",
    "    id_vars=[\"Ethnoculture\", \"Year\"],\n",
    "    var_name=\"Ward_ID\",\n",
    "    value_name=\"Population\",\n",
    ")\n",
    "ethnicity_data2021 = pd.melt(\n",
    "    ethnicity_data2021,\n",
    "    id_vars=[\"Ethnoculture\", \"Year\"],\n",
    "    var_name=\"Ward_ID\",\n",
    "    value_name=\"Population\",\n",
    ")\n",
    "columns_order = [col for col in ethnicity_data2016.columns if col != \"Year\"] + [\"Year\"]\n",
    "ethnicity_data2016 = ethnicity_data2016[columns_order]\n",
    "ethnicity_data2021 = ethnicity_data2021[columns_order]\n",
    "\n",
    "# merge the datasets\n",
    "EthnoculturalDimension = pd.concat(\n",
    "    [ethnicity_data2016, ethnicity_data2021], ignore_index=True\n",
    ")\n",
    "\n",
    "# change data types\n",
    "EthnoculturalDimension[\"Population\"] = EthnoculturalDimension[\"Population\"].astype(int)\n",
    "EthnoculturalDimension[\"Ward_ID\"] = EthnoculturalDimension[\"Ward_ID\"].astype(str)\n",
    "EthnoculturalDimension[\"Ethnoculture\"] = EthnoculturalDimension[\"Ethnoculture\"].astype(str)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "# HOUSEHOLD DIMENSION\n",
    "household_data2016 = pd.read_csv(\n",
    "    os.path.join(data_sources,\"WardProfile2016.csv\"),\n",
    "    skiprows=range(98),\n",
    "    nrows=9,\n",
    "    header=0,\n",
    "    encoding=\"ISO-8859-1\",\n",
    "    low_memory=False,\n",
    ")\n",
    "household_data2021 = pd.read_csv(\n",
    "    os.path.join(data_sources,\"WardProfile2021.csv\"),\n",
    "    skiprows=range(108),\n",
    "    nrows=9,\n",
    "    header=0,\n",
    "    encoding=\"ISO-8859-1\",\n",
    "    low_memory=False,\n",
    ")\n",
    "\n",
    "# Filter out rows where 'Household' column is not NA\n",
    "household_data2016 = household_data2016[household_data2016[\"Household\"].notna()]\n",
    "household_data2021 = household_data2021[household_data2021[\"Household\"].notna()]\n",
    "\n",
    "# change value of one of income ranges\n",
    "household_data2016.loc[\n",
    "    household_data2016[\"Household\"].str.contains(\n",
    "        \"Total - Private households by household\"\n",
    "    ),\n",
    "    \"Household\",\n",
    "] = \"Total Household\"\n",
    "household_data2021.loc[\n",
    "    household_data2021[\"Household\"].str.contains(\n",
    "        \"Total - Private households by household\"\n",
    "    ),\n",
    "    \"Household\",\n",
    "] = \"Total Household\"\n",
    "\n",
    "\n",
    "# Removing the spaces before each household type\n",
    "household_data2016[\"Household\"] = household_data2016[\"Household\"].str.strip()\n",
    "household_data2021[\"Household\"] = household_data2021[\"Household\"].str.strip()\n",
    "\n",
    "# Melt the DataFrame to get 'Ward_ID', 'Household', and 'Population' columns\n",
    "household_data2016[\"Year\"] = 2016\n",
    "household_data2021[\"Year\"] = 2021\n",
    "household_data2016 = pd.melt(\n",
    "    household_data2016,\n",
    "    id_vars=[\"Household\", \"Year\"],\n",
    "    var_name=\"Ward_ID\",\n",
    "    value_name=\"Population\",\n",
    ")\n",
    "household_data2021 = pd.melt(\n",
    "    household_data2021,\n",
    "    id_vars=[\"Household\", \"Year\"],\n",
    "    var_name=\"Ward_ID\",\n",
    "    value_name=\"Population\",\n",
    ")\n",
    "columns_order = [col for col in household_data2016.columns if col != \"Year\"] + [\"Year\"]\n",
    "household_data2016 = household_data2016[columns_order]\n",
    "household_data2021 = household_data2021[columns_order]\n",
    "\n",
    "# Merge the datasets\n",
    "HouseholdDimension = pd.concat(\n",
    "    [household_data2016, household_data2021], ignore_index=True\n",
    ")\n",
    "\n",
    "\n",
    "# change data types\n",
    "HouseholdDimension[\"Ward_ID\"] = HouseholdDimension[\"Ward_ID\"].astype(str)\n",
    "HouseholdDimension[\"Household\"] = HouseholdDimension[\"Household\"].astype(str)\n",
    "\n",
    "# rename column\n",
    "HouseholdDimension.rename(columns={\"Household\": \"Household_Description\"}, inplace=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/73/svsm8vw94tzb_w86qcpwfh2h0000gn/T/ipykernel_98687/742526377.py:109: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n",
      "  ShelterDimension[\"Average_Monthly_Shelter_Costs\"]\n"
     ]
    }
   ],
   "source": [
    "# SHELTER DIMENSION\n",
    "# 2021\n",
    "# Load the CSV file\n",
    "df = pd.read_csv(os.path.join(data_sources,\"WardProfile2021.csv\"), encoding=\"ISO-8859-1\", low_memory=False)\n",
    "\n",
    "# Define the indices\n",
    "tenant_costs_index = 1373\n",
    "owner_costs_index = 1377\n",
    "\n",
    "tenant_percent_spending = 1374\n",
    "owner_percent_spending = 1378\n",
    "\n",
    "tenant_households = 1372\n",
    "owner_households = 1376\n",
    "\n",
    "# create the Ward_IDs list based on the column headers\n",
    "ward_ids = df.columns[1:]\n",
    "\n",
    "tenant_data = []\n",
    "owner_data = []\n",
    "\n",
    "for i, ward_id in enumerate(ward_ids):\n",
    "    tenant_data.append(\n",
    "        {\n",
    "            \"Household_Type\": \"Tenant\",\n",
    "            \"Ward_ID\": ward_id,\n",
    "            \"Total_Households\": df.iloc[tenant_households, i + 1],\n",
    "            \"Average_Monthly_Shelter_Costs\": df.iloc[tenant_costs_index, i + 1],\n",
    "            \"Percent_Spending_30_Percent_Or_More_On_Shelter\": df.iloc[\n",
    "                tenant_percent_spending, i + 1\n",
    "            ],\n",
    "        }\n",
    "    )\n",
    "    owner_data.append(\n",
    "        {\n",
    "            \"Household_Type\": \"Owner\",\n",
    "            \"Ward_ID\": ward_id,\n",
    "            \"Total_Households\": df.iloc[owner_households, i + 1],\n",
    "            \"Average_Monthly_Shelter_Costs\": df.iloc[owner_costs_index, i + 1],\n",
    "            \"Percent_Spending_30_Percent_Or_More_On_Shelter\": df.iloc[\n",
    "                owner_percent_spending, i + 1\n",
    "            ],\n",
    "        }\n",
    "    )\n",
    "\n",
    "# Combine the tenant and owner data\n",
    "combined_data = tenant_data + owner_data\n",
    "# print(combined_data[:20])\n",
    "# 2016\n",
    "# Load the CSV file\n",
    "df1 = pd.read_csv(os.path.join(data_sources,\"WardProfile2021.csv\"), encoding=\"ISO-8859-1\", low_memory=False)\n",
    "\n",
    "# Define the indices\n",
    "tenant_costs_index = 1239\n",
    "owner_costs_index = 1243\n",
    "\n",
    "tenant_percent_spending = 1240\n",
    "owner_percent_spending = 1244\n",
    "\n",
    "tenant_households = 1238\n",
    "owner_households = 1242\n",
    "\n",
    "# Create the Ward_IDs list based on the column headers\n",
    "ward_ids = df1.columns[1:]\n",
    "\n",
    "tenant_data = []\n",
    "owner_data = []\n",
    "\n",
    "for i, ward_id in enumerate(ward_ids):\n",
    "    tenant_data.append(\n",
    "        {\n",
    "            \"Household_Type\": \"Tenant\",\n",
    "            \"Ward_ID\": ward_id,\n",
    "            \"Total_Households\": df1.iloc[tenant_households, i + 1],\n",
    "            \"Average_Monthly_Shelter_Costs\": df1.iloc[tenant_costs_index, i + 1],\n",
    "            \"Percent_Spending_30_Percent_Or_More_On_Shelter\": df1.iloc[\n",
    "                tenant_percent_spending, i + 1\n",
    "            ],\n",
    "        }\n",
    "    )\n",
    "    owner_data.append(\n",
    "        {\n",
    "            \"Household_Type\": \"Owner\",\n",
    "            \"Ward_ID\": ward_id,\n",
    "            \"Total_Households\": df1.iloc[owner_households, i + 1],\n",
    "            \"Average_Monthly_Shelter_Costs\": df1.iloc[owner_costs_index, i + 1],\n",
    "            \"Percent_Spending_30_Percent_Or_More_On_Shelter\": df1.iloc[\n",
    "                owner_percent_spending, i + 1\n",
    "            ],\n",
    "        }\n",
    "    )\n",
    "# Combine the tenant and owner data\n",
    "combined_data2 = tenant_data + owner_data\n",
    "# print(combined_data2[:20])\n",
    "# create the DataFrame\n",
    "ShelterDimension2016 = pd.DataFrame(combined_data2)\n",
    "ShelterDimension2021 = pd.DataFrame(combined_data)\n",
    "\n",
    "ShelterDimension2016[\"Year\"] = 2016\n",
    "ShelterDimension2021[\"Year\"] = 2021\n",
    "\n",
    "# merge the datasets\n",
    "ShelterDimension = pd.concat(\n",
    "    [ShelterDimension2016, ShelterDimension2021], ignore_index=True\n",
    ")\n",
    "\n",
    "# clean data and change data types\n",
    "ShelterDimension[\"Average_Monthly_Shelter_Costs\"] = (\n",
    "    ShelterDimension[\"Average_Monthly_Shelter_Costs\"]\n",
    "    .str.replace(\"$\", \"\")\n",
    "    .str.replace(\",\", \"\")\n",
    "    .astype(int)\n",
    ")\n",
    "ShelterDimension[\"Percent_Spending_30_Percent_Or_More_On_Shelter\"] = (\n",
    "    ShelterDimension[\"Percent_Spending_30_Percent_Or_More_On_Shelter\"]\n",
    "    .str.replace(\"%\", \"\")\n",
    "    .astype(float)\n",
    ")\n",
    "ShelterDimension[\"Total_Households\"] = ShelterDimension[\"Total_Households\"].astype(int)\n",
    "ShelterDimension[\"Household_Type\"] = ShelterDimension[\"Household_Type\"].astype(str)\n",
    "ShelterDimension[\"Ward_ID\"] = ShelterDimension[\"Ward_ID\"].astype(str)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
